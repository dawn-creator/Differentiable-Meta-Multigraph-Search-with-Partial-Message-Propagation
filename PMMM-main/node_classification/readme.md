## File Description

`data/`: Input data.

`preprocess.py`: The script for data preprocessing.

`model_search_paths.py`: The DAG search space (i.e., the super-net).

`train_search.py`: The script to perform search. Please run `python train_search.py --help` to see command-line arguments and what they mean.

`model_paths.py`: The wrap of an arbitrary architecture derived from the search space.

`train.py`: The script to train the discovered architectures from scratch. Please run `python train.py --help` to see command-line arguments and what they mean.

`arch.py`: The architectures we use for evaluation are recorded here and they are imported by `train.py`.

## Preparing Data

We use the preprocessed datasets provided by [GTN](https://github.com/seongjunyun/Graph_Transformer_Networks). Download data from [here](https://drive.google.com/file/d/1iV-WZWias6rVZxfz5-tjympj615hDWWe/view?usp=share_link) and put DBLP, ACM and IMDB under `data`. Then, run:

```shell
python preprocess.py
```

Take DBLP as an example to show the formats of input data:

`node_features.pkl` is a numpy array whose shape is (num_of_nodes, num_of_features). It contains input node features.

`edges.pkl` is a list of scipy sparse matrices. Each matrix has a shape of (num_of_nodes, num_of_nodes) and is formed by edges of a certain edge type.

`labels.pkl` is a list of lists. labels[0] is a list containing training labels and each item in it has the form [node_id, target]. labels[1] and labels[2] are validation labels and test labels respectively with the same format.

`node_types.npy` is generated by `preprocess.py`. It is a numpy array which contains node type information and has a shape of (num_of_nodes,). Each value in the array is an integer and lies in [0, num_of_node_types).

Note that the inputs of our method are only raw information of a heterogeneous network (network topology, node types, edge types, and node attributes if applicable). We do not need to manually design any meta path or meta graph.

 ## Search

To obtain the architectures we use in the paper, run:

```shell
python train_search.py --dataset DBLP --steps 4 --eps 0 --seed 0 --lam_seq 0.9 --lam_res 0.9
python train_search.py --dataset ACM --steps 4 --eps 0.3 --seed 2 --lam_seq 0.8 --lam_res 0.8
python train_search.py --dataset IMDB --steps 4 --eps 0.3 --seed 1 --lam_seq 0.9 --lam_res 0.8 --k 2
```

Logs are automatically generated under `log/search/` with the following format:

```
epoch number; training error; validation error; architecture derived at the end of this epoch
```

To obtain a good architecture, we usually need to run the search algorithm several times with different random seeds.

## Architecture Interpretation

Suppose the number of intermediate states in a meta graph is K. An encoding of a meta graph consists of two lists. The first list is of length K and contains indexes of selected edge types between state (i - 1) and state i (1 <= i <= K). The second list is of length K(K-1)/2 and contains indexes of selected edge types between state j and state i (0 <= j < i - 1, 2 <= i <= K). You can refer to the function `parse` defined in `model_paths.py` for how an encoding is obtained based on architecture parameters. For search and evaluation, the mapping between edge types and indexes should be consistent so that an encoding is able to be correctly recognized. 

## Evaluation
For Multigraph, run the following commands to train the derived architectures from scratch:

```shell
python train.py --dataset DBLP --lr 0.004 --wd 0.001 --seed 1 --dropout 0.5 --arch DBLP_M
python train.py --dataset ACM --lr 0.0005 --wd 0.1 --dropout 0.4  --no_norm --in_nl --arch ACM_M
python train.py --dataset IMDB --lr 0.01 --wd 0.02 --seed 30 --dropout 0.5 --ratio 0.1 --arch IMDB_M 
```

For PMMM, run the following commands to train the derived architectures from scratch:

```shell
python train.py --dataset DBLP --lr 0.003 --wd 0.002 --dropout 0.5 --seed 6 --ratio 0.8 --arch DBLP 
python train.py --dataset ACM --lr 0.002 --wd 0.2 --dropout 0.1 --seed 26 --ratio 0.8 --no_norm --in_nl --arch ACM 
python train.py --dataset IMDB --lr 0.02 --wd 0.01 --dropout 0.5 --seed 0 --ratio 0.1 --arch IMDB 
```

Logs are automatically generated under `log/eval/` with the following format:

```
epoch number; training error; validation error
```
